Total training examples: 24773
Total validation examples: 2517
/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.
Loss at iteration 10 : 0.4498482346534729
Loss at iteration 20 : 0.4283871352672577
Loss at iteration 30 : 0.43393632769584656
Loss at iteration 40 : 0.35417017340660095
Loss at iteration 50 : 0.3092530071735382
Loss at iteration 60 : 0.3899848461151123
Loss at iteration 70 : 0.322383314371109
Loss at iteration 80 : 0.41211897134780884
Loss at iteration 90 : 0.361880898475647
Loss at iteration 100 : 0.3656955063343048
Loss at iteration 110 : 0.3431398272514343
Loss at iteration 120 : 0.27385246753692627
Loss at iteration 130 : 0.26967623829841614
Loss at iteration 140 : 0.26477038860321045
Loss at iteration 150 : 0.1923588365316391
Loss at iteration 160 : 0.21465657651424408
Loss at iteration 170 : 0.1608959287405014
Loss at iteration 180 : 0.1895250827074051
Loss at iteration 190 : 0.134504497051239
Loss at iteration 200 : 0.14463123679161072
Loss at iteration 210 : 0.1030818521976471
Loss at iteration 220 : 0.13114701211452484
Loss at iteration 230 : 0.07755187898874283
Loss at iteration 240 : 0.08999470621347427
Loss at iteration 250 : 0.10072791576385498
Loss at iteration 260 : 0.01768452301621437
Loss at iteration 270 : 0.008884204551577568
Loss at iteration 280 : 0.02059340290725231
Loss at iteration 290 : 0.01625337265431881
Loss at iteration 300 : 0.023608481511473656
Loss at iteration 310 : 0.02560351975262165
Loss at iteration 320 : 0.011964842677116394
Loss at iteration 330 : 0.010936100035905838
Loss at iteration 340 : 0.013498516753315926
Loss at iteration 350 : 0.013291224837303162
Loss at iteration 360 : 0.027828514575958252
Loss at iteration 370 : 0.012999376282095909
Loss at iteration 380 : 0.013087174855172634
Loss at iteration 390 : 0.037000302225351334
Loss at iteration 400 : 0.03919197991490364
Loss at iteration 410 : 0.013962013646960258
Loss at iteration 420 : 0.01325318031013012
Loss at iteration 430 : 0.01646449789404869
Loss at iteration 440 : 0.009978828020393848
Loss at iteration 450 : 0.017976701259613037
Loss at iteration 460 : 0.0155313266441226
Loss at iteration 470 : 0.015639469027519226
Loss at iteration 480 : 0.013334278017282486
Loss at iteration 490 : 0.04417756199836731
Loss at iteration 500 : 0.019041545689105988
Loss at iteration 510 : 0.014281915500760078
Loss at iteration 520 : 0.022322924807667732
Loss at iteration 530 : 0.024687260389328003
Loss at iteration 540 : 0.020377373322844505
Loss at iteration 550 : 0.018064232543110847
Loss at iteration 560 : 0.014683584682643414
Loss at iteration 570 : 0.03480619564652443
Loss at iteration 580 : 0.024098940193653107
Loss at iteration 590 : 0.01570906490087509
Loss at iteration 600 : 0.011617620475590229
Loss at iteration 610 : 0.021502047777175903
Loss at iteration 620 : 0.017682170495390892
Loss at iteration 630 : 0.018240079283714294
Loss at iteration 640 : 0.02272968553006649
Loss at iteration 650 : 0.013213912025094032
Loss at iteration 660 : 0.02718542516231537
Loss at iteration 670 : 0.009807192720472813
Loss at iteration 680 : 0.022194739431142807
Loss at iteration 690 : 0.015375476330518723
Loss at iteration 700 : 0.023675143718719482
Loss at iteration 710 : 0.03415203094482422
Loss at iteration 720 : 0.014427004382014275
Loss at iteration 730 : 0.0176241435110569
Loss at iteration 740 : 0.03426293656229973
Loss at iteration 750 : 0.018211152404546738
Loss at iteration 760 : 0.018311982974410057
Loss at iteration 770 : 0.022548945620656013
Loss at iteration 780 : 0.011919918470084667
Loss at iteration 790 : 0.016996337100863457
Loss at iteration 800 : 0.025753078982234
Loss at iteration 810 : 0.014890512451529503
Loss at iteration 820 : 0.020524289458990097
Loss at iteration 830 : 0.009440558031201363
Loss at iteration 840 : 0.019458377733826637
Loss at iteration 850 : 0.026836900040507317
Loss at iteration 860 : 0.026373766362667084
Loss at iteration 870 : 0.016783975064754486
Loss at iteration 880 : 0.016117658466100693
Loss at iteration 890 : 0.02471148781478405
Loss at iteration 900 : 0.016444526612758636
Loss at iteration 910 : 0.009120296686887741
Loss at iteration 920 : 0.019018802791833878
Loss at iteration 930 : 0.03684563562273979
Loss at iteration 940 : 0.024096312001347542
Loss at iteration 950 : 0.011668497696518898
Loss at iteration 960 : 0.015258291736245155
Loss at iteration 970 : 0.0183743666857481
Loss at iteration 980 : 0.01852942630648613
Loss at iteration 990 : 0.026822835206985474
Loss at iteration 1000 : 0.009661504998803139
Loss at iteration 1010 : 0.008849789388477802
Loss at iteration 1020 : 0.020896494388580322
Loss at iteration 1030 : 0.0190279483795166
Loss at iteration 1040 : 0.014032440260052681
Loss at iteration 1050 : 0.025719068944454193
Loss at iteration 1060 : 0.03410305455327034
Loss at iteration 1070 : 0.010308622382581234
Loss at iteration 1080 : 0.01932954229414463
Loss at iteration 1090 : 0.018346738070249557
Loss at iteration 1100 : 0.014377365820109844
Loss at iteration 1110 : 0.02687087655067444
Loss at iteration 1120 : 0.022557182237505913
Loss at iteration 1130 : 0.012549872510135174
Loss at iteration 1140 : 0.013251311145722866
Loss at iteration 1150 : 0.04025052860379219
Loss at iteration 1160 : 0.02044219896197319
Loss at iteration 1170 : 0.022992515936493874
Loss at iteration 1180 : 0.01917094551026821
Loss at iteration 1190 : 0.015977999195456505
Loss at iteration 1200 : 0.013225129805505276
Loss at iteration 1210 : 0.024164602160453796
Loss at iteration 1220 : 0.02079121582210064
Loss at iteration 1230 : 0.009167802520096302
Loss at iteration 1240 : 0.01587396115064621
Loss at iteration 1250 : 0.02434411272406578
Loss at iteration 1260 : 0.02345973625779152
Loss at iteration 1270 : 0.0238800048828125
Loss at iteration 1280 : 0.01569809578359127
Loss at iteration 1290 : 0.01376116368919611
Loss at iteration 1300 : 0.013293635100126266
Loss at iteration 1310 : 0.020213687792420387
Loss at iteration 1320 : 0.027872418984770775
Loss at iteration 1330 : 0.01912965625524521
Loss at iteration 1340 : 0.009797802194952965
Loss at iteration 1350 : 0.012504014186561108
Loss at iteration 1360 : 0.02295464091002941
Loss at iteration 1370 : 0.017194444313645363
Loss at iteration 1380 : 0.017615240067243576
Loss at iteration 1390 : 0.025743721053004265
Loss at iteration 1400 : 0.02226162701845169
Loss at iteration 1410 : 0.017129313200712204
Loss at iteration 1420 : 0.03241806477308273
Loss at iteration 1430 : 0.020386144518852234
Loss at iteration 1440 : 0.02266281098127365
Loss at iteration 1450 : 0.018954230472445488
Loss at iteration 1460 : 0.00916358083486557
Loss at iteration 1470 : 0.032288312911987305
Loss at iteration 1480 : 0.017924612388014793
Loss at iteration 1490 : 0.02058182656764984
Loss at iteration 1500 : 0.019643748179078102
Loss at iteration 1510 : 0.011491839773952961
Loss at iteration 1520 : 0.024684449657797813
Loss at iteration 1530 : 0.01691298745572567
Loss at iteration 1540 : 0.013944178819656372
Loss at iteration 1550 : 0.02613006718456745
Loss at iteration 1560 : 0.020172812044620514
Loss at iteration 1570 : 0.022359369322657585
Loss at iteration 1580 : 0.025068063288927078
Loss at iteration 1590 : 0.015457048080861568
Loss at iteration 1600 : 0.021734217181801796
Loss at iteration 1610 : 0.013450917787849903
Loss at iteration 1620 : 0.013216269202530384
Loss at iteration 1630 : 0.010038013570010662
Loss at iteration 1640 : 0.01322421245276928
Loss at iteration 1650 : 0.010159264318645
Loss at iteration 1660 : 0.020451372489333153
Loss at iteration 1670 : 0.020128339529037476
Loss at iteration 1680 : 0.01630958542227745
Loss at iteration 1690 : 0.016353413462638855
Loss at iteration 1700 : 0.026308929547667503
Loss at iteration 1710 : 0.03193877264857292
Loss at iteration 1720 : 0.020840300247073174
Loss at iteration 1730 : 0.016458574682474136
Loss at iteration 1740 : 0.017922887578606606
Loss at iteration 1750 : 0.015786120668053627
Loss at iteration 1760 : 0.03154563531279564
Loss at iteration 1770 : 0.013993704691529274
Loss at iteration 1780 : 0.016973132267594337
Loss at iteration 1790 : 0.014579210430383682
Loss at iteration 1800 : 0.014495057053864002
Loss at iteration 1810 : 0.014492976479232311
Loss at iteration 1820 : 0.02424727752804756
Loss at iteration 1830 : 0.027751626446843147
Loss at iteration 1840 : 0.01947205327451229
Loss at iteration 1850 : 0.023902274668216705
Loss at iteration 1860 : 0.009316444396972656
Loss at iteration 1870 : 0.027618175372481346
Loss at iteration 1880 : 0.00868943240493536
Loss at iteration 1890 : 0.018922962248325348
Loss at iteration 1900 : 0.023969225585460663
Loss at iteration 1910 : 0.012495871633291245
Loss at iteration 1920 : 0.014966643415391445
Loss at iteration 1930 : 0.02124669775366783
Loss at iteration 1940 : 0.010780850425362587
Loss at iteration 1950 : 0.012656153179705143
Loss at iteration 1960 : 0.028571411967277527
Loss at iteration 1970 : 0.01588706485927105
Loss at iteration 1980 : 0.014074605889618397
Loss at iteration 1990 : 0.02012544870376587
Loss at iteration 2000 : 0.02455475553870201
Loss at iteration 2010 : 0.01679975353181362
Loss at iteration 2020 : 0.030848108232021332
Loss at iteration 2030 : 0.021167421713471413
Loss at iteration 2040 : 0.02149597927927971
Loss at iteration 2050 : 0.015962056815624237
Loss at iteration 2060 : 0.030209317803382874
Loss at iteration 2070 : 0.018518077209591866
Loss at iteration 2080 : 0.02547038160264492
Loss at iteration 2090 : 0.011736774817109108
Loss at iteration 2100 : 0.029438626021146774
Loss at iteration 2110 : 0.030195094645023346
Loss at iteration 2120 : 0.027264634147286415
Loss at iteration 2130 : 0.01041432935744524
Loss at iteration 2140 : 0.01707254722714424
Loss at iteration 2150 : 0.013704566285014153
Loss at iteration 2160 : 0.02408486232161522
Loss at iteration 2170 : 0.013230188749730587
Loss at iteration 2180 : 0.026539722457528114
Loss at iteration 2190 : 0.014519927091896534
Loss at iteration 2200 : 0.01499875821173191
Loss at iteration 2210 : 0.01995600387454033
Loss at iteration 2220 : 0.01556223351508379
Loss at iteration 2230 : 0.009336388669908047
Loss at iteration 2240 : 0.02036569081246853
Loss at iteration 2250 : 0.009475266560912132
Loss at iteration 2260 : 0.029340842738747597
Loss at iteration 2270 : 0.016338739544153214
Loss at iteration 2280 : 0.025987589731812477
Loss at iteration 2290 : 0.017431722953915596
Loss at iteration 2300 : 0.008989116176962852
Loss at iteration 2310 : 0.02461116760969162
Loss at iteration 2320 : 0.011866283603012562
Loss at iteration 2330 : 0.03905588760972023
Loss at iteration 2340 : 0.020130565389990807
Loss at iteration 2350 : 0.016677143052220345
Loss at iteration 2360 : 0.021896641701459885
Loss at iteration 2370 : 0.01416347548365593
Loss at iteration 2380 : 0.023951981216669083
Loss at iteration 2390 : 0.021714206784963608
Loss at iteration 2400 : 0.016988249495625496
Loss at iteration 2410 : 0.015307293273508549
Loss at iteration 2420 : 0.029720274731516838
Loss at iteration 2430 : 0.010339136235415936
Loss at iteration 2440 : 0.008753257803618908
Loss at iteration 2450 : 0.014563232660293579
Loss at iteration 2460 : 0.018840651959180832
Loss at iteration 2470 : 0.02218758501112461
Loss at iteration 2480 : 0.015344965271651745
Loss at iteration 2490 : 0.01932193711400032
Loss at iteration 2500 : 0.01810164749622345
Loss at iteration 2510 : 0.014484205283224583
Loss at iteration 2520 : 0.03318237513303757
Loss at iteration 2530 : 0.03311627730727196
Loss at iteration 2540 : 0.03036155365407467
Loss at iteration 2550 : 0.013091211207211018
Loss at iteration 2560 : 0.01895863562822342
Loss at iteration 2570 : 0.021838802844285965
Loss at iteration 2580 : 0.0452156737446785
Loss at iteration 2590 : 0.021226489916443825
Loss at iteration 2600 : 0.01899120584130287
Loss at iteration 2610 : 0.020878570154309273
Loss at iteration 2620 : 0.029678698629140854
Loss at iteration 2630 : 0.015849098563194275
Loss at iteration 2640 : 0.022756345570087433
Loss at iteration 2650 : 0.00999769102782011
Loss at iteration 2660 : 0.022092698141932487
Loss at iteration 2670 : 0.03208641707897186
Loss at iteration 2680 : 0.01984037272632122
Loss at iteration 2690 : 0.017068304121494293
Loss at iteration 2700 : 0.018173977732658386
Loss at iteration 2710 : 0.02370629645884037
Loss at iteration 2720 : 0.01437271386384964
Loss at iteration 2730 : 0.018191179260611534
Loss at iteration 2740 : 0.01516922377049923
Loss at iteration 2750 : 0.015031164512038231
Loss at iteration 2760 : 0.013669312931597233
Loss at iteration 2770 : 0.018788522109389305
Loss at iteration 2780 : 0.010998588055372238
Loss at iteration 2790 : 0.02650902234017849
Loss at iteration 2800 : 0.01651725545525551
Loss at iteration 2810 : 0.02258823998272419
Loss at iteration 2820 : 0.017208077013492584
Loss at iteration 2830 : 0.006720527075231075
Loss at iteration 2840 : 0.019663166254758835
Loss at iteration 2850 : 0.01023012399673462
Loss at iteration 2860 : 0.01319032721221447
Loss at iteration 2870 : 0.027913551777601242
Loss at iteration 2880 : 0.035648807883262634
Loss at iteration 2890 : 0.013770021498203278
Loss at iteration 2900 : 0.020253775641322136
Loss at iteration 2910 : 0.019597778096795082
Loss at iteration 2920 : 0.024624045938253403
Loss at iteration 2930 : 0.025156723335385323
Loss at iteration 2940 : 0.015911921858787537
Loss at iteration 2950 : 0.013808725401759148
Loss at iteration 2960 : 0.034287989139556885
Loss at iteration 2970 : 0.012457691133022308
Loss at iteration 2980 : 0.013786734081804752
Loss at iteration 2990 : 0.014614186249673367
Loss at iteration 3000 : 0.02184753492474556
Loss at iteration 3010 : 0.01701102964580059
Loss at iteration 3020 : 0.03668689355254173
Loss at iteration 3030 : 0.03220004588365555
Loss at iteration 3040 : 0.0161343552172184
Loss at iteration 3050 : 0.03816450759768486
Loss at iteration 3060 : 0.014327340759336948
Loss at iteration 3070 : 0.025695471093058586
Loss at iteration 3080 : 0.01436858344823122
Loss at iteration 3090 : 0.025248833000659943